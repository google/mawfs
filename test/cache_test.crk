# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import crack.cmdline CmdOptions, Option, CMD_BOOL;
import crack.enc.base64 altEncode;  # For debugging hash values.
import crack.cont.array Array;
import crack.hash.sha256 SHA256;
import crack.io cerr, FStr, StringWriter;
import crack.protobuf ProtoWriter;
import crack.cont.hashmap HashMap;
import crack.lang AppendBuffer, ManagedBuffer;
import crack.logger debug, setLogLevel, DEBUG;
import mawfs.blockstore Commit, Entry, Node, NodeStore, MODE_DIR;
import mawfs.memstore MemNodeStore;
import mawfs.cache Cache, CachedNode;
import mawfs.testutil copy, makeRandomData;

@import crack.ann assert, impl;

opts := CmdOptions![
    Option('fast', 'f', 'Run only fast tests', 'false', CMD_BOOL)
];
opts.parse();

fastTests := opts.getBool('fast');


Node makeFileNode(String contents, int32 mode) {
    node := Node();
    node.contents = contents;
    node.size = contents ? node.contents.count() : 0;
    node.mode = mode;

    return node;
}

Node makeDirNode() {
    node := Node();
    node.mode = MODE_DIR;
    return node;
}


AppendBuffer readAll(CachedNode cached) {
    AppendBuffer readData = {4096};
    ManagedBuffer temp = {256};
    uint64 pos;
    while (rc := cached.read(pos, temp)) {
        pos += rc;
        readData.extend(temp);
    }
    return readData;
}

setLogLevel(DEBUG);

node := makeFileNode('contents of foo', 0);

nodeStore := MemNodeStore();
digest := nodeStore.storeNode(node);

root := Node();
node.children = Array[Entry]();
entry := Entry();
entry.hash = digest;
entry.name = 'foo';
root.children_append(entry);

digest = nodeStore.storeNode(root);
nodeStore.storeRoot(digest);

# NOTE: We should be passing a commit digest to this, but the root digest
# (or any other non-empty string) should work just as well for this exercise.
cache := Cache(nodeStore, 'master', digest);
cachedRoot := CachedNode(cache, digest, root);
cachedNode := cachedRoot.getChild(0);
@assert(cachedNode.getContents() == 'contents of foo');

# Do it again, make sure this works right if the object is loaded.
cachedNode = cachedRoot.getChild(0);
@assert(cachedNode.getContents() == 'contents of foo');

# Add a child.
node = makeFileNode('contents of bar', 0);
cachedRoot.addChild('bar', node);

@assert(cachedRoot.getChild(0).getContents() == 'contents of bar');
@assert(cachedRoot.getChild(1).getContents() == 'contents of foo');

# Verify that the insertion order is correct.
cachedRoot.addChild('cat', makeFileNode('cat contents', 0));
cachedRoot.addChild('alpha', makeFileNode('alpha contents', 0));
cachedRoot.addChild('zeta', makeFileNode('zeta contents', 0));

# Rewrite "foo" to test overwrites.
cachedRoot.addChild('foo', makeFileNode('foo contents', 0));
cachedRoot.addChild('zzdir', makeDirNode());
zzdir := cachedRoot.getChild(5);
zzdir.addChild('zzfile', makeFileNode('zzfile contents', 0));
zzdir.addChild('deleted', makeFileNode('deleted file', 0));
zzdir.deleteChild('deleted');

# Test a write in a subdirectory (propagation code was broken).
zzfile := cachedRoot.getChild('zzdir').getChild('zzfile');
zzfile.write(7, 'moar data');

cachedRoot.addChild('zzzshrunk', makeFileNode('original file', 0));
cachedRoot.getChild('zzzshrunk').resize(5);
@assert(cachedRoot.getChild('zzzshrunk').getContents() == 'origi');

cachedRoot.addChild('zzzenlarged', makeFileNode('original file', 0));
cachedRoot.getChild('zzzenlarged').resize(16);
@assert(cachedRoot.getChild('zzzenlarged').getContents() ==
         'original file\0\0\0'
        );

cachedRoot.addChild('zzztruncated', makeFileNode('original file', 0));
cachedRoot.getChild('zzztruncated').resize(0);
@assert(!cachedRoot.getChild('zzztruncated').getContents());

cachedRoot.addChild('zzzwasempty', makeFileNode(null, 0));
cachedRoot.getChild('zzzwasempty').resize(5);
@assert(cachedRoot.getChild('zzzwasempty').getContents() == '\0\0\0\0\0');

void verifyContents() {
    @assert(cachedRoot.getChild(0).getContents() == 'alpha contents');
    @assert(cachedRoot.getChild(1).getContents() == 'contents of bar');
    @assert(cachedRoot.getChild(2).getContents() == 'cat contents');
    @assert(cachedRoot.getChild(3).getContents() == 'foo contents');
    @assert(cachedRoot.getChild(4).getContents() == 'zeta contents');
    @assert(cachedRoot.getChild(5).getMode() == MODE_DIR);
    @assert(cachedRoot.getChild(5).getChild(0).getContents() ==
            'zzfile moar data');
    @assert(cachedRoot.getChild(5).getChildCount() == 1);
    @assert(cachedRoot.getChild('zzzshrunk').getContents() == 'origi');
    @assert(cachedRoot.getChild('zzzenlarged').getContents() ==
             'original file\0\0\0'
            );
    @assert(!cachedRoot.getChild('zzztruncated').getContents());
    @assert(cachedRoot.getChild('zzzwasempty').getContents() == '\0\0\0\0\0');
}

# TODO: test committing changes at this point as well as after replaying them
# back from the cache.

# Test reconstructing changes from cache.
cachedRoot = CachedNode(cache, digest, nodeStore.getRoot());
cachedRoot.replayJournal();
verifyContents();

# Test committing changes.
nodeStore.storeRoot(digest = cachedRoot.commit());
cachedRoot = CachedNode(cache, digest, nodeStore.getRoot());
verifyContents();

# Test a write.
cachedRoot.addChild('written', Node());
cached := cachedRoot.getChild('written');
cached.write(0, 'first data');
if (true) {
    buf := ManagedBuffer(1024);
    cached.read(0, buf);
    @assert(buf == 'first data');
}
@assert(cached.getSize() == 10);

# Test appending children.
if (true) {
    cerr `appending master\n`;
    cache := Cache(nodeStore, 'master', digest);
    cache.maxChildren = 4;
    CachedNode root = {cache, digest, Node()};

    AppendBuffer originalData = {4096};
    for (int i = 0; i < 100; ++i) {
        line := FStr() `this is $i\n`;
        root.append(line);
        originalData.extend(line);
        size := root.verify();
    }

    @assert(readAll(root) == originalData);
}

# Case 1: verify that we can insert objects into an empty node.
if (true) {
    cerr `inserting objects into an empty node\n`;
    cache := Cache(nodeStore, 'master', digest);
    CachedNode root = {cache, digest, Node()};
    randomData := makeRandomData();
    root.write(0, randomData);

    # Make sure we have a nice tree.
    root.verify();
}

# TODO: verify that we can insert at an offset in an empty node and that we
# get filled in with zeroes.

# Write a large amount of random data into the middle of a node with contents.
if (true) {
    cerr `write random data into the middle of a content node.\n`;
    cache := Cache(nodeStore, 'master', digest);
    CachedNode root = {cache, digest, Node()};
    root.node.contents = 'this is a test';

    randomData := String(makeRandomData(), true);
    root.write(8, randomData);

    @assert(readAll(root) == 'this is ' + randomData);
    root.verify();
}

# Write into the middle of a node with contents.
if (true) {
    cerr `write simple data into the middle of a content node.\n`;
    cache := Cache(nodeStore, 'master', digest);
    CachedNode root = {cache, digest, Node()};
    root.node.contents = 'this is a test';

    root.write(5, 'crzy');

    @assert(readAll(root) == 'this crzy test');
    root.verify();
}

# Write data after the end of a node with children.
if (true) {
    cerr `write data after the end of a node with children\n`;
    cache := Cache(nodeStore, 'master', digest);
    CachedNode root = {cache, digest, Node()};

    # populate an empty node.
    data := String(makeRandomData(), true);
    root.write(0, data);

    # Write beyond the end.
    root.write(data.size + 1000, data);

    @assert(readAll(root) == data + String(1000, 0) + data);
}

# Write a large amount of random data into a node with children.
if (false) {
    cerr `write random data into a node with children\n`;
    cache := Cache(nodeStore, 'master', digest);
    CachedNode root = {cache, digest, Node()};

    # populate an empty node.
    data := String(makeRandomData(), true);
    root.write(0, data);

    # Insert random data somewhere.
    root.write(25000, data);
}

CachedNode makeSmallFileTree() {
    cache := Cache(nodeStore, 'master', digest);
    cache.maxChildren = 3;
    CachedNode root = {cache, digest, Node()};
    root.append('alpha');
    root.append('bravo');
    root.append('charlie');
    root.append('delta');
    root.append('echo');

    return root;
}

# Unit tests for node merge.
if (true) {
    cerr `delete nodes\n`;
    root := makeSmallFileTree();

    # The root should have two children, one with three content nodes and one
    # with two.
    @assert(root.getChildCount() == 2);
    root.commit();

    root.deleteNode(10); # Delete "charlie".
    root.deleteNode(5);  # Delete "bravo".

    # Verify that the nodes have been coalesced.
    @assert(root.getChildCount() == 3);
    @assert(root.getContents() == 'alphadeltaecho');

    root.verify();

    root = makeSmallFileTree();

    root.deleteNode(10); # Delete "charlie".
    root.deleteNode(10); # Delete "delta".

    @assert(root.getChildCount() == 3);
    @assert(root.getContents() == 'alphabravoecho');
}

if (true) {
    cerr `deleteSpan:\n  partial contents\n`;
    root := makeSmallFileTree();
    root.deleteSpan(1, 4);
    root.verify();
    @assert(root.getContents() == 'aabravocharliedeltaecho')

    #   Deleting a single child
    cerr `  delete first child\n`;
    root = makeSmallFileTree();
    root.deleteSpan(0, 5);
    root.verify();
    @assert(root.getContents() == 'bravocharliedeltaecho');

    cerr `  delete last child\n`;
    root = makeSmallFileTree();
    root.deleteSpan(22, 27);
    root.verify();
    @assert(root.getContents() == 'alphabravocharliedelta');

    cerr `  delete multiple children\n`;
    root = makeSmallFileTree();
    root.deleteSpan(0, 10);
    root.verify();
    @assert(root.getContents() == 'charliedeltaecho');

    cerr `  delete span crossing multiple children\n`;
    root = makeSmallFileTree();
    root.deleteSpan(1, 9);
    root.verify();
    @assert(root.getContents() == 'aocharliedeltaecho');

    cerr `  force merge to previous\n`;
    root = makeSmallFileTree();
    root.deleteSpan(10, 24);
    root.verify();
    @assert(root.getContents() == 'alphabravoho');
    # Make sure we collapsed the top tier after merging.
    @assert(root.getChildCount() == 3);
}

if (true) {
    cerr `writes into the middle of a file.\n`;

    # Store an initial commit.  This test does a lot of writes, and the
    # journal quickly grows to 16M, so commits happen.
    commit := Commit();
    commit.root = digest;
    commitDigest := nodeStore.storeCommit(commit);
    nodeStore.setHead('master', commitDigest);

    root := CachedNode(cache, digest, Node());
    randomData := String(makeRandomData(), true);
    root.write(0, randomData);
    root.write(500, randomData);
    @assert(root.getContents() == randomData.substr(0, 500) + randomData);

    # Test writing to the middle of a chunk.
    root = CachedNode(cache, digest, Node());
    root.write(0, randomData);
    root.write(10, randomData.substr(0, 20));
    @assert(root.getContents().substr(0, 30) ==
             randomData.substr(0, 10) + randomData.substr(0, 20)
            );
    @assert(root.getContents() ==
             randomData.substr(0, 10) + randomData.substr(0, 20) +
             randomData.substr(30)
            );

    # Test writing in the fingerprint zone of a chunk.
    root = CachedNode(cache, digest, Node());
    root.write(0, randomData);
    subseq := randomData.substr(0, root.getChunks()[0].size - 20);
    root.write(10, subseq);
    @assert(root.getContents() ==
             randomData.substr(0, 10) + subseq +
             randomData.substr(10 + subseq.size)
            );

    # Test writing into the beginning of the second chunk.
    root = CachedNode(cache, digest, Node());
    root.write(0, randomData);
    firstChunkSize := root.getChunks()[0].size;
    root.write(firstChunkSize, 'stuff');

    String snippet(String data) {
        return data.substr(0, 10).getRepr();
    }
    contents := root.getContents();
    @assert(root.getContents() == randomData.substr(0, firstChunkSize) +
                                  'stuff' +
                                  randomData.substr(firstChunkSize + 5)
            );

    if (true) {
        # Test modifying the fingerprint at the end of a node boundary.
        cache = Cache(nodeStore, 'master', digest);
        cache.maxChildren = 3;
        root = CachedNode(cache, digest, Node());
        root.write(0, randomData);

        # We should have two leafs grouped under an intermediate node.
        chunks := root.getChunks();
        boundary := chunks[0].size + chunks[1].size;
        cerr `modifying at the end of a root boundary\n`;
        root.write(boundary - 32, 'xx');

        contents := root.getContents();
        root.verify();
        @assert(root.getContents() == randomData.substr(0, boundary - 32) +
                                      'xx' +
                                      randomData.substr(boundary - 30)
                );
    }

    if (!fastTests) {
        cerr `boundary tests\n`;
        # Test the exciting boundary conditions for all existing chunks.
        # failed at 2170, 17614, 63393, 66881, 66903
        uint64 startOff, endOff;
        root = CachedNode(cache, digest, Node());
        root.write(0, randomData);
        chunks := root.getChunks();
        const EMPTY := randomData.substr(0, 0);
        for (int i; i < chunks.count(); ++i) {
            startChunk := chunks[i];
            int ixi;
            for (ix :in Array[int]![startOff, startOff + 1,
                                    startOff + startChunk.size - 32,
                                    startOff + startChunk.size - 31
                                    ]
                ) {

                cerr `$((i * 4 + ixi) * 100 / (chunks.count() * 4))% `;
                ixi++;
                endOff = 0;
                for (int j; j < chunks.count(); ++j) {
                    endChunk := chunks[j];
                    for (jx :in Array[int]![endOff - 31, endOff - 32,
                                            endOff + endChunk.size - 32,
                                            endOff + endChunk.size - 31,
                                            endOff + endChunk.size,
                                            endOff + endChunk.size + 1
                                            ]
                        ) {
                        if (i + ix <= j + jx) {
                            root = CachedNode(cache, digest, Node());
                            root.write(0, randomData);
                            start := uint64(ix);
                            size := uint64(jx - start);
                            if (size > randomData.size)
                                size = randomData.size;
                            slice := randomData.substr(0, size);
                            root.write(start, slice);
                            @assert(root.getContents() ==
                                    randomData.substr(0, start) + slice +
                                    (start + size < randomData.size ?
                                        randomData.substr(start + size) :
                                        EMPTY
                                    )
                                    );
                        }
                    }

                    endOff += endChunk.size;
                }
            }

            startOff += startChunk.size;
        }
    }

    if (true) {
        cerr `resize to larger\n`;
        root = CachedNode(cache, digest, Node());
        root.write(0, randomData);
        cerr `size is $(randomData.size + 1000)\n`;
        root.resize(randomData.size + 1000);
        @assert(root.getContents() == randomData + String(1000, 0));
    }

    if (true) {
        cerr `resize to smaller\n`;
        root.write(0, randomData);
        root.resize(randomData.size - 1000);
        @assert(root.getContents() ==
                 randomData.substr(0, randomData.size - 1000)
                );
    }
}

## Writes 'data' into 'node' one small chunk at a time.
void writeChunked(CachedNode node, String data, uint64 chunkSize) {
    uint64 i = 0;
    while (i < data.size) {
        size := data.size - i;
        size = (size > chunkSize) ? chunkSize : size;
        node.write(i, data.substr(i, size));
        i += chunkSize;
    }
}

cerr `appends merge last chunk\n`;
if (true) {
    # Write the entire buffer in one fell swoop.
    randomData := String(makeRandomData(), true);
    root := CachedNode(cache, digest, Node());
    root.write(0, randomData);
    chunks := root.getChunks();
    contents := root.getContents();

    # Now write it one page at a time.
    root = CachedNode(cache, digest, Node());
    writeChunked(root, randomData, 4096);

    # Chunking should be the same.
    @assert(contents == root.getContents());
    @assert(chunks == root.getChunks());
}

import crack.io Formatter;
void writeChunks(Formatter out, Array[String] chunks) {
    for (chunk :in chunks)
        out `$(chunk.size), `;
    cerr `\n`;
}

cerr `verify auto-commit\n`;
if (true) {
    root := CachedNode(cache, digest, Node());

    # Store an initial commit.
    commit := Commit();
    commit.root = digest;
    commitDigest := nodeStore.storeCommit(commit);
    nodeStore.setHead('master', commitDigest);

    cache.maxJournalSize = 4096;
    randomData := String(makeRandomData());
    writeChunked(root, randomData, 1024);
    @assert(cache.store.getJournalSize('master') < 4096);
    @assert(root.getContents() == randomData);

    # Verify that we can load it back up.
    commitDigest = nodeStore.getHead('master');
    commit = nodeStore.getCommit(commitDigest);
    cache.baselineCommit = commitDigest;
    root = CachedNode(cache, commit.root, nodeStore.getNode(commit.root));
    root.replayJournal();
    @assert(root.getContents() == randomData);
}

cerr `commit of unmaterialized node\n`;
if (true) {
    cache = Cache(nodeStore, 'master', digest);
    root := CachedNode(cache, digest, Node());

    # Store an initial commit.
    commit := Commit();
    commit.root = digest;
    commitDigest := nodeStore.storeCommit(commit);
    nodeStore.setHead('master', commitDigest);

    # Add some content.
    root.addChild('a', makeFileNode('contents of a', 0));
    root.addChild('b', makeFileNode('contents of b', 0));
    root.commitTree();

    # reload.
    commitDigest = nodeStore.getHead('master');
    commit = nodeStore.getCommit(commitDigest);
    root = CachedNode(cache, commit.root, nodeStore.getNode(commit.root));

    root.addChild('c', makeFileNode('contents of c', 0));
    root.commitTree();
}

# TODO:
#   - create a directory and then a child node to verify that when we do
#     addChild() on a node without a digest the change propagates back to the
#     parent.

cerr `ok\n`;
